{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM51UVmFnBuA"
      },
      "outputs": [],
      "source": [
        "### Set up ----\n",
        "\n",
        "# install any missing libraries\n",
        "\n",
        "# !pip install palmerpenguins\n",
        "\n",
        "# load libraries\n",
        "import pandas as pd # data frame to store the data\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score # for evaluation metrics\n",
        "\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import seaborn as sns # for plotting pairs plot\n",
        "\n",
        "import numpy as np # many mathematical operations and more, used for sqrt()\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Load Data ----\n",
        "## load the palmer penguins dataset from a CSV file"
      ],
      "metadata": {
        "id": "WJI1WR5pQ1Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data pre-processing ----\n",
        "# Drop rows with missing values for simplicity\n",
        "\n",
        "## FILL IN WITH YOUR CODE HERE\n",
        "\n",
        "# Create categorical variable(s)\n",
        "penguins['sex'] = pd.Categorical(penguins['sex'])\n",
        "\n",
        "penguins = pd.get_dummies(penguins, columns=['sex'], drop_first=True)\n",
        "\n",
        "# pd.get_dummies() creates a boolean variable, sex_male,\n",
        "# we need it to be numeric\n",
        "penguins['sex_male'] = penguins['sex_male'].astype(int)\n",
        "\n",
        "\n",
        "## NOW DO THE SAME FOR SPECIES. NOTE THAT IT HAS 3 LEVELS,\n",
        "## UNLIKE SEX, WHICH HAS 2. PRINT THE DATA TO LOOK AT IT FIRST\n",
        "\n"
      ],
      "metadata": {
        "id": "qipUoCHToRHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Setp 2: Create a train/test split ----\n",
        "## Divide 30% of your data off to a test set\n",
        "## save the test set as test_data\n",
        "## Save the training set as train_data\n",
        "## Set a random state of 8675309, for reproducibility\n",
        "\n"
      ],
      "metadata": {
        "id": "H3fuE8WInYYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Data Exploration ----\n",
        "## Create a pairs plot on the numeric variables, colored by sex\n",
        "## Bonus: you could also color by Island and Species\n"
      ],
      "metadata": {
        "id": "vXiUSgkioUK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4: Feature Engineering ----\n",
        "\n",
        "# pull out predictor variables\n",
        "# (so we don't accidentally include something not helpful)\n",
        "predictors = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'sex_male']\n",
        "\n",
        "# separate X and Y variables\n",
        "X_train = train_data[predictors].copy()\n",
        "y_train = train_data['body_mass_g'].copy()\n",
        "\n",
        "X_test = test_data[predictors].copy()\n",
        "y_test = test_data['body_mass_g']\n",
        "\n",
        "## Don't add quadratic terms. Let's see how linear does.\n"
      ],
      "metadata": {
        "id": "D_11jf0Mol-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5: Feature & Model Selection ----\n",
        "## build a model will all variables\n"
      ],
      "metadata": {
        "id": "_9caTr4posy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Do backwards selection on your model\n",
        "## FILL IN THE MISSING ARGUMENTS TO DO BACKWARDS SELECTION\n",
        "sfs_backward = SFS(LinearRegression(), # type of model\n",
        "                  k_features=(1, 8), # will find between 1 and 8 features\n",
        "                  # forward=, # search direction (backwards if False)\n",
        "                  # floating=, # controls stepwise vs. one direction\n",
        "                  scoring='neg_root_mean_squared_error', # find max RMSE\n",
        "                  cv=5) # how many random subsets of data to test\n",
        "\n",
        "# Get selected features\n",
        "sfs_backward = sfs_backward.fit(X_train, y_train)\n",
        "\n",
        "best_subset = sfs_backward.subsets_[1]\n",
        "for v in sfs_backward.subsets_.values():\n",
        "    if v['avg_score'] > best_subset['avg_score']:\n",
        "        best_subset = v\n",
        "\n",
        "\n",
        "print(f'Best score: {- best_subset[\"avg_score\"]:.2f}')\n",
        "print(f'Best subset (indices): {best_subset[\"feature_idx\"]}')\n",
        "print(f'Best subset (names): {best_subset[\"feature_names\"]}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Fx_eOoT0o3yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Do stepwise selection starting with your kitchen sink model\n",
        "## FILL IN THE MISSING ARGUMENTS TO DO STEPWISE SELECTION\n",
        "sfs_step = SFS(LinearRegression(), # type of model\n",
        "                  k_features=(1, 8), # will find between 1 and 8 features\n",
        "                  # forward=, # search direction (backwards if False)\n",
        "                  # floating=, # controls stepwise vs. one direction\n",
        "                  scoring='neg_root_mean_squared_error', # find max RMSE\n",
        "                  cv=5) # how many random subsets of data to test\n",
        "\n",
        "sfs_step = sfs_step.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features\n",
        "best_subset = sfs_step.subsets_[1]\n",
        "for v in sfs_step.subsets_.values():\n",
        "    if v['avg_score'] > best_subset['avg_score']:\n",
        "        best_subset = v\n",
        "\n",
        "\n",
        "print(f'Fest score: {- best_subset[\"avg_score\"]:.2f}')\n",
        "print(f'Best subset (indices): {best_subset[\"feature_idx\"]}')\n",
        "print(f'Best subset (names): {best_subset[\"feature_names\"]}')"
      ],
      "metadata": {
        "id": "vD_cY7W7o5j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FIT YOUR FINAL MODEL USING THE STATSMODELS PACKAGE\n"
      ],
      "metadata": {
        "id": "l0Q1A9_Vo7lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 6: Predictions and Final Evaluation ----\n",
        "\n",
        "## GET PREDICTIONS WITH THE STATSMODELS PACKAGE\n",
        "## EVALUATE THE OUT-OF-SAMPLE R-SQUARED\n",
        "\n",
        "## PLOT PREDICTED VS. ACTUAL VALUES ON THE TEST SET"
      ],
      "metadata": {
        "id": "AtOwllamn3OJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
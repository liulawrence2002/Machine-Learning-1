{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmAW84sTFaH7"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Step 0: Setup\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Uncomment and adjust to install libraries not available on Google Colab\n",
        "# !pip install packagename\n",
        "\n",
        "# Load any libraries used for this project\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_curve,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# for stepwise selection later\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "from google.colab import files  # so we can upload a CSV to our notebook\n",
        "\n",
        "uploaded = files.upload() # opens a file chooser to select our csv file\n"
      ],
      "metadata": {
        "id": "GMmoWw3SFyBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penguins = pd.read_csv(\"PalmerPenguins.csv\")"
      ],
      "metadata": {
        "id": "miJm9fl-F2wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Pre-processing ----\n",
        "\n",
        "# check data structure\n",
        "\n",
        "# any obvious pre-formatting, do here (e.g., numeric to categorical etc.)\n",
        "\n",
        "# Initial structure of the data\n",
        "print(\"Structure / dtypes:\")\n",
        "print(penguins.info())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(penguins.isna().sum())\n"
      ],
      "metadata": {
        "id": "CY_UZF0WGKZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's drop rows with missing values\n",
        "penguins = penguins.dropna()\n",
        "\n",
        "# Let's turn sex into a binary variable\n",
        "penguins[\"sex\"] = penguins[\"sex\"].map({\"male\": 1, \"female\": 0})"
      ],
      "metadata": {
        "id": "g3g2fFHfGWRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2: Split data into train and test sets ----\n",
        "\n",
        "train, test = train_test_split(\n",
        "    penguins,\n",
        "    test_size=0.30,\n",
        "    random_state=90210\n",
        "    )\n"
      ],
      "metadata": {
        "id": "eHhjI9J1GcK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEUzwfR_Gyds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Exploratory data analysis ----\n",
        "\n",
        "# Summarize and visualize your *training* data\n",
        "# Do so to understand the data set and inform both\n",
        "# model selection and feature engineering\n",
        "\n",
        "# summary statistics\n",
        "print(\"\\nTrain summary:\")\n",
        "print(train.describe())\n",
        "\n",
        "# From past experience, here are our numeric predictors\n",
        "predictors = [\n",
        "    'bill_length_mm',\n",
        "    'bill_depth_mm',\n",
        "    'flipper_length_mm',\n",
        "    'body_mass_g'\n",
        "]\n",
        "\n",
        "# Pairs plot of our data, colored by our outcome variable\n",
        "sns.pairplot(\n",
        "    penguins[predictors + ['sex']].assign(\n",
        "        sex = train['sex'].astype(\"category\")\n",
        "    ),\n",
        "    hue='sex',\n",
        "    diag_kind='kde'\n",
        "    )\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wf7jJZB2xNyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "1. Build an initial model of all variables.\n",
        "2. Plot a ROC curve and calculate AUC in-sample, based on the training data\n",
        "3. Use stepwise regression (that's both directions) to select variables for a\n",
        "    second model\n",
        "4. Get AUC and plot a ROC curve for both models (if they are different)\n",
        "    on your test data\n",
        "5. Choose the model you think is better.\n",
        "6. Calculate exact marginal effects of the odds.\n",
        "7. Calculate average marginal effects of the probability of 'male'\n",
        "8. Pick the variable that has the highest MFX in odds and probabilites.\n",
        "    Are they the same variable?\n",
        "9. For each, write a sentence in plain english interpreting the marginal effect\n",
        "    of that variable on the odds/probability of a penguin being male."
      ],
      "metadata": {
        "id": "TWiE_URXIpDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4: Feature engineering ----\n",
        "\n",
        "# Any variable transformations, creation of new variables etc.\n",
        "# goes here. Also imputing missing values etc.\n",
        "# Don't forget to apply it to your test set too.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bPSBnZq9QitC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5: Feature and Model selection ----\n",
        "\n",
        "# Build different models, select different hyperparameters, validate the models\n",
        "# etc. here. Steps 4 and 5 are usually iterative until you've settled on a model\n",
        "# or models that you're comfortable with.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ebkjZ6qzQ2Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 6: Predictions and final model evaluation ----\n",
        "\n",
        "# Predict on your test set to accurately measure out-of-sample performance\n",
        "# Past this point, you need to report results back in the real world\n"
      ],
      "metadata": {
        "id": "y6mDN1bBIWlQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
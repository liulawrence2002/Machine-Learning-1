{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsmHPwGBy5D6"
      },
      "source": [
        "# Cross validation, imputation, and feature engineering\n",
        "\n",
        "Below sets up our examples. We will be learning the following:\n",
        "\n",
        "1. For cross validation:\n",
        "    - `StratifiedKFold` for classification or  `KFold` for numeric prediction for set up\n",
        "    - `cross_val_score` to run the validation.\n",
        "2. For imputation:\n",
        "    - `SimpleImputer` for simple imputations\n",
        "    - `KNNImputer` for a model-based imputation of numeric variables\n",
        "3. For feature engineering:\n",
        "    - `StandardScaler` for standardization\n",
        "    - `MinMaxScaler` for minmax normalization\n",
        "    - `FunctionTransformer` for custom functions applied to columns\n",
        "4. We'll learn to set up pipelines, which will simplify our code, rather than having to manually apply each step to the training and test sets.\n",
        "    - `Pipeline` for pipeline steps\n",
        "    - `ColumnTransformer` to pull our pipeline steps together into a single function to call on each data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wEcndjku3gdW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# standard modeling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for cross validation\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Imputation and feature engineering\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# For pipelines, make feature engineering and imputation more straightforward\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XdVgPi3n8QqY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 344 entries, 0 to 343\n",
            "Data columns (total 7 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   species            344 non-null    object \n",
            " 1   island             344 non-null    object \n",
            " 2   bill_length_mm     342 non-null    float64\n",
            " 3   bill_depth_mm      342 non-null    float64\n",
            " 4   flipper_length_mm  342 non-null    float64\n",
            " 5   body_mass_g        342 non-null    float64\n",
            " 6   sex                333 non-null    object \n",
            "dtypes: float64(4), object(3)\n",
            "memory usage: 18.9+ KB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "species               0\n",
              "island                0\n",
              "bill_length_mm        2\n",
              "bill_depth_mm         2\n",
              "flipper_length_mm     2\n",
              "body_mass_g           2\n",
              "sex                  11\n",
              "dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "penguins = sns.load_dataset(\"penguins\")  # built into seaborn\n",
        "\n",
        "penguins.head()\n",
        "penguins.info()\n",
        "penguins.isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k49N8HOL8H-b"
      },
      "source": [
        "You should see (approx):\n",
        "\n",
        "* sex: 11 missing\n",
        "* bill_length_mm: 2 missing\n",
        "* bill_depth_mm: 2 missing\n",
        "* flipper_length_mm: 2 missing\n",
        "* body_mass_g: 2 missing\n",
        "\n",
        "For this module, let’s:\n",
        "\n",
        "Predict species (multiclass classification)\n",
        "\n",
        "Use numeric features + sex as predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dca-fu0z8mmf"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "    \"bill_length_mm\",\n",
        "    \"bill_depth_mm\",\n",
        "    \"flipper_length_mm\",\n",
        "    \"body_mass_g\",\n",
        "    \"sex\"\n",
        "]\n",
        "target = \"species\"\n",
        "\n",
        "# Drop rows where target is missing (should be none, but be safe)\n",
        "penguins = penguins.dropna(subset=[target]).copy()\n",
        "\n",
        "# establish matrix of predictors and target variable\n",
        "X = penguins[features]\n",
        "y = (penguins[target] == \"Chinstrap\").astype(int) # binary classification, chinstrap vs. all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EmZEz-v_2qx"
      },
      "source": [
        "# Simple K-Fold Cross Validation\n",
        "\n",
        "Done only with complete cases and numeric features to  keep things simple for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OnQcsUmqADBV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV accuracy scores: [0.97101449 0.98550725 0.97058824 1.         0.98529412]\n",
            "Mean accuracy: 0.9824808184143222\n"
          ]
        }
      ],
      "source": [
        "# Use only numeric features to start\n",
        "num_features = [\n",
        "    \"bill_length_mm\",\n",
        "    \"bill_depth_mm\",\n",
        "    \"flipper_length_mm\",\n",
        "    \"body_mass_g\"\n",
        "]\n",
        "\n",
        "X_num = penguins[num_features]\n",
        "\n",
        "# Drop rows with numeric missing to keep it simple for this first demo\n",
        "non_na_indicator = ~X_num.isna().any(axis=1) # flags rows without missing values\n",
        "\n",
        "# Breakdown of above:\n",
        "# ~ means \"not\" (reverses True/False of booleans, below)\n",
        "# X_num.isna() returns a matrix of True/False where X_num has/has not na values\n",
        "# .any(axis=1) means any True value row-wise, converting to a vector\n",
        "\n",
        "X_num_complete = X_num[non_na_indicator]\n",
        "\n",
        "y_complete = y[non_na_indicator]\n",
        "\n",
        "# Define model object\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "# 5-fold Stratified CV (preserve class proportions)\n",
        "cv = StratifiedKFold( # sets up CV object\n",
        "    n_splits=5, # how many folds?\n",
        "    shuffle=True, # True means stratify by the class variable\n",
        "    random_state=42\n",
        "    )\n",
        "\n",
        "# Note: if we were doing numeric prediction, e.g., predicting body_mass_g\n",
        "# as we did in earlier weeks, we would use the function KFold instead of\n",
        "# StratifiedKFold. Its arguments are the same, but knowing which you need\n",
        "# will prevent errors when calling cross_val_score, below.\n",
        "\n",
        "scores = cross_val_score( # actual sampling and training done here\n",
        "    log_reg, # defined model object\n",
        "    X_num_complete, # x variable\n",
        "    y_complete, # y variable (used for stratification)\n",
        "    cv=cv, # cv object\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "print(\"CV accuracy scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEFysYwKELU7"
      },
      "source": [
        "# Standardization & normalization\n",
        "\n",
        "Now we will:\n",
        "* Fit the scaler on train only\n",
        "* Transform both train and test\n",
        "\n",
        "As a reminder, we need to fit the scaler on the train only. (Think of it as a little model.) We do this because the mean and standard deviation (or min and max for normalization) are pulled from the data. If we do this on the whole dataset, we have information from the test set leaking into training, meaning our estimate of out-of-sample performance would be overly optimistic.\n",
        "\n",
        "This problem *does* affect our evaluation using K-fold cross validation. But we are going to accept it because it's still a useful comparison between models, even if it's not a fully fair evaluation of out-of-sample performance.\n",
        "\n",
        "One could get more complicated and put the feature engineering step *inside* of a cross validation loop. But I leave that to you later when your Python programming is stronger.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3kCPkmhPEKrl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUC (standardized): 0.875\n"
          ]
        }
      ],
      "source": [
        "### Standardization ----\n",
        "\n",
        "# Do train/test split before feature engineering\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_num_complete,\n",
        "    y_complete,\n",
        "    test_size=0.3,\n",
        "    stratify=y_complete,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Standardization: (x - mean) / std\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_std = scaler.fit_transform(X_train)   # fit on train\n",
        "X_test_std  = scaler.transform(X_test)       # transform test\n",
        "\n",
        "# fit a model\n",
        "log_reg = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train_std, y_train)\n",
        "\n",
        "# get predictions and test\n",
        "y_pred = log_reg.predict(X_test_std)\n",
        "\n",
        "print(\"Test AUC (standardized):\", roc_auc_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hyguTliGEyX8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUC (min–max): 0.7\n"
          ]
        }
      ],
      "source": [
        "### Normalization ----\n",
        "minmax = MinMaxScaler()\n",
        "\n",
        "X_train_mm = minmax.fit_transform(X_train)\n",
        "X_test_mm  = minmax.transform(X_test)\n",
        "\n",
        "# fit a model\n",
        "log_reg_mm = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "log_reg_mm.fit(X_train_mm, y_train)\n",
        "\n",
        "# get predictions and test\n",
        "y_pred_mm = log_reg_mm.predict(X_test_mm)\n",
        "\n",
        "print(\"Test AUC (min–max):\", roc_auc_score(y_test, y_pred_mm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1MmZM7CEKhF"
      },
      "source": [
        "# Simple imputation (mean/median + most_frequent)\n",
        "\n",
        "Now we bring back the missing values and show imputation.\n",
        "\n",
        "Simple imputation replaces missing values with either the mean or median of the non-missing values of numeric variables or it replaces missing values with the most frequent category for categorical variables.\n",
        "\n",
        "Please note: imputing missing values is not without hazzard. It can alter the distribution of your data and, again, cause a model to fail spectacularly on new data.\n",
        "\n",
        "Like standardization and normalization, we can think of imputation as a mini model, as it requires statistics from the training set to implement. So by \"training\" an imputer on training data and then applying it to the test set for prediction, we get a fair evaluation of our modeling *process* on held out data.\n",
        "\n",
        "There are more elaborate packages and methods for imputation. But this is good for a first course. I leave the rest to your Googling skills. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3UavdSc9FGUV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUC (median imputation + standardization): 0.8809523809523809\n"
          ]
        }
      ],
      "source": [
        "### Numeric-only: median imputation + standardization ----\n",
        "\n",
        "# train test split FIRST\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# define imputer and scaler\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit imputer and scaler on training data only\n",
        "X_train[num_features] = num_imputer.fit_transform(X_train[num_features])\n",
        "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
        "\n",
        "# Now apply to test data\n",
        "X_test[num_features]  = num_imputer.transform(X_test[num_features])\n",
        "X_test[num_features]  = scaler.transform(X_test[num_features])\n",
        "\n",
        "# train a model\n",
        "log_reg = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train[num_features], y_train)\n",
        "\n",
        "# Predict and eval on test set\n",
        "y_pred = log_reg.predict(X_test[num_features])\n",
        "\n",
        "print(\"Test AUC (median imputation + standardization):\", roc_auc_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iPYfWZH_QbfC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39.5</td>\n",
              "      <td>17.4</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>3250.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36.7</td>\n",
              "      <td>19.3</td>\n",
              "      <td>193.0</td>\n",
              "      <td>3450.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>46.8</td>\n",
              "      <td>14.3</td>\n",
              "      <td>215.0</td>\n",
              "      <td>4850.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>50.4</td>\n",
              "      <td>15.7</td>\n",
              "      <td>222.0</td>\n",
              "      <td>5750.0</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>45.2</td>\n",
              "      <td>14.8</td>\n",
              "      <td>212.0</td>\n",
              "      <td>5200.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>49.9</td>\n",
              "      <td>16.1</td>\n",
              "      <td>213.0</td>\n",
              "      <td>5400.0</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n",
              "0              39.1           18.7              181.0       3750.0    Male\n",
              "1              39.5           17.4              186.0       3800.0  Female\n",
              "2              40.3           18.0              195.0       3250.0  Female\n",
              "3               NaN            NaN                NaN          NaN     NaN\n",
              "4              36.7           19.3              193.0       3450.0  Female\n",
              "..              ...            ...                ...          ...     ...\n",
              "339             NaN            NaN                NaN          NaN     NaN\n",
              "340            46.8           14.3              215.0       4850.0  Female\n",
              "341            50.4           15.7              222.0       5750.0    Male\n",
              "342            45.2           14.8              212.0       5200.0  Female\n",
              "343            49.9           16.1              213.0       5400.0    Male\n",
              "\n",
              "[344 rows x 5 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4myR2iCmK2P7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>37.8</td>\n",
              "      <td>17.1</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3300.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>43.5</td>\n",
              "      <td>14.2</td>\n",
              "      <td>220.0</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>41.1</td>\n",
              "      <td>19.1</td>\n",
              "      <td>188.0</td>\n",
              "      <td>4100.0</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>45.3</td>\n",
              "      <td>13.8</td>\n",
              "      <td>208.0</td>\n",
              "      <td>4200.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>52.5</td>\n",
              "      <td>15.6</td>\n",
              "      <td>221.0</td>\n",
              "      <td>5450.0</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>46.2</td>\n",
              "      <td>17.5</td>\n",
              "      <td>187.0</td>\n",
              "      <td>3650.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>43.5</td>\n",
              "      <td>15.2</td>\n",
              "      <td>213.0</td>\n",
              "      <td>4650.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>35.7</td>\n",
              "      <td>16.9</td>\n",
              "      <td>185.0</td>\n",
              "      <td>3150.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>36.6</td>\n",
              "      <td>18.4</td>\n",
              "      <td>184.0</td>\n",
              "      <td>3475.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>47.5</td>\n",
              "      <td>14.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>4875.0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n",
              "10             37.8           17.1              186.0       3300.0  Female\n",
              "288            43.5           14.2              220.0       4700.0  Female\n",
              "67             41.1           19.1              188.0       4100.0    Male\n",
              "280            45.3           13.8              208.0       4200.0  Female\n",
              "301            52.5           15.6              221.0       5450.0    Male\n",
              "..              ...            ...                ...          ...     ...\n",
              "193            46.2           17.5              187.0       3650.0  Female\n",
              "332            43.5           15.2              213.0       4650.0  Female\n",
              "60             35.7           16.9              185.0       3150.0  Female\n",
              "147            36.6           18.4              184.0       3475.0  Female\n",
              "308            47.5           14.0              212.0       4875.0  Female\n",
              "\n",
              "[240 rows x 5 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Categorical imputation (for sex) and including it as a feature ----\n",
        "\n",
        "# Now include sex as a categorical feature and impute its missing values with the most frequent category.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define Imputers\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "\n",
        "# Train imputers and apply to training set only\n",
        "# Numeric columns\n",
        "X_train[num_features] = num_imputer.fit_transform(X_train[num_features])\n",
        "\n",
        "# Categorical columns\n",
        "# Note: double brackets required because a single column gets converted to\n",
        "# a series, not a one-column data frame. The double brackets enforce the\n",
        "# data frame structure.\n",
        "X_train[['sex']] = cat_imputer.fit_transform(X_train[['sex']])\n",
        "\n",
        "# Apply to test set\n",
        "X_test[num_features]  = num_imputer.transform(X_test[num_features])\n",
        "X_test[['sex']]  = cat_imputer.transform(X_test[['sex']])\n",
        "\n",
        "\n",
        "# Note: Sex is still a string {'male', 'female'}\n",
        "# We need to convert it to 0/1 to use it for training\n",
        "X_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cbKpIjcvLZsJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>37.8</td>\n",
              "      <td>17.1</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3300.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>43.5</td>\n",
              "      <td>14.2</td>\n",
              "      <td>220.0</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>41.1</td>\n",
              "      <td>19.1</td>\n",
              "      <td>188.0</td>\n",
              "      <td>4100.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>45.3</td>\n",
              "      <td>13.8</td>\n",
              "      <td>208.0</td>\n",
              "      <td>4200.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>52.5</td>\n",
              "      <td>15.6</td>\n",
              "      <td>221.0</td>\n",
              "      <td>5450.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>46.2</td>\n",
              "      <td>17.5</td>\n",
              "      <td>187.0</td>\n",
              "      <td>3650.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>43.5</td>\n",
              "      <td>15.2</td>\n",
              "      <td>213.0</td>\n",
              "      <td>4650.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>35.7</td>\n",
              "      <td>16.9</td>\n",
              "      <td>185.0</td>\n",
              "      <td>3150.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>36.6</td>\n",
              "      <td>18.4</td>\n",
              "      <td>184.0</td>\n",
              "      <td>3475.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>47.5</td>\n",
              "      <td>14.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>4875.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  sex\n",
              "10             37.8           17.1              186.0       3300.0    0\n",
              "288            43.5           14.2              220.0       4700.0    0\n",
              "67             41.1           19.1              188.0       4100.0    0\n",
              "280            45.3           13.8              208.0       4200.0    0\n",
              "301            52.5           15.6              221.0       5450.0    0\n",
              "..              ...            ...                ...          ...  ...\n",
              "193            46.2           17.5              187.0       3650.0    0\n",
              "332            43.5           15.2              213.0       4650.0    0\n",
              "60             35.7           16.9              185.0       3150.0    0\n",
              "147            36.6           18.4              184.0       3475.0    0\n",
              "308            47.5           14.0              212.0       4875.0    0\n",
              "\n",
              "[240 rows x 5 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# One-hot encode sex\n",
        "X_train['sex'] = (X_train['sex'] == \"male\").astype(int)\n",
        "X_test['sex']  = (X_test['sex'] == \"male\").astype(int)\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mCVijJdsJr2K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUC (numeric + sex, imputed): 0.8809523809523809\n"
          ]
        }
      ],
      "source": [
        "# Let's also scale numeric columns\n",
        "scaler = StandardScaler()\n",
        "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
        "X_test[num_features]  = scaler.transform(X_test[num_features])\n",
        "\n",
        "# Train a classifier\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions and evaluate on test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "print(\"Test AUC (numeric + sex, imputed):\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDXep1taMuo1"
      },
      "source": [
        "# Pipelines: impute → scale → encode → model\n",
        "\n",
        "Without pipelines, on a sufficiently large and complex data set, you'll end up with a lot to manage. Imagine if you wanted to normalize some columns, standardize others, and leave others alone.\n",
        "\n",
        "Pipelines make managing this complexity easier by wrapping all of our data transformation steps into a single function, trained on training data, that can be applied to both train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gaPzwpj5Mx_Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline CV accuracy scores: [0.95652174 0.97101449 0.97101449 0.98550725 0.97058824]\n",
            "Mean accuracy: 0.970929241261722\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a pipeline step for numeric variables,\n",
        "# doing both standardization and imputation at once\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Do the same for categorical variables\n",
        "\n",
        "# First, define a function to encode a categorical variable to binary\n",
        "def encode_sex(X):\n",
        "    # X is a 2D array of shape (n_samples, 1)\n",
        "    # Compare to \"male\" elementwise, get True/False, cast to int (1/0)\n",
        "    return (X == \"male\").astype(int)\n",
        "\n",
        "# now, define the pipeline step with imputation and encoding\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode_sex\", FunctionTransformer(encode_sex))\n",
        "])\n",
        "\n",
        "# Now, define our single function that can be applied at once\n",
        "# for numeric and categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, num_features),\n",
        "        (\"cat\", categorical_transformer, ['sex'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", LogisticRegression(\n",
        "        max_iter=1000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# K-fold CV with *no leakage* – all preprocessing inside each fold\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(\n",
        "    clf,\n",
        "    X,\n",
        "    y,\n",
        "    cv=cv,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "print(\"Pipeline CV accuracy scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tmLkeGORXB"
      },
      "source": [
        "# Model-based imputation (rough & ready): KNNImputer\n",
        "So far, we only imputed median (or mean) values for numeric columns. We can be smarter. `KNNImputer` uses the K-nearest-neighbor model (a very simple predictive model based on nearby points) to predict what a missing value should be based on the values of other non-missing predictors for each observation. This should make us more confident in imputed values for numeric variables.\n",
        "\n",
        "(One could, in principle, do the same for categorical variables. But we'd need to use a different Python package and introduce too much complexity for now. So, exercise left to your Google-fu once more.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aKsQK8VvOUJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUC with KNN imputation: 0.8809523809523809\n"
          ]
        }
      ],
      "source": [
        "# train test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# define our knn imputer and apply it to numeric columns\n",
        "imputer_knn = KNNImputer(n_neighbors=5)\n",
        "\n",
        "X_train[num_features] = imputer_knn.fit_transform(X_train[num_features])\n",
        "X_test[num_features] = imputer_knn.transform(X_test[num_features])\n",
        "\n",
        "# standardize variables as well\n",
        "scaler = StandardScaler()\n",
        "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
        "X_test[num_features]  = scaler.transform(X_test[num_features])\n",
        "\n",
        "# train model\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train[num_features], y_train)\n",
        "\n",
        "# predict and evaluate on test set\n",
        "y_pred = log_reg.predict(X_test[num_features])\n",
        "\n",
        "print(\"Test AUC with KNN imputation:\", roc_auc_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1MuDATlObhS"
      },
      "source": [
        "If you want it inside a pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bb0lROz9OXw5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN imputation + pipeline CV accuracy: 0.9833333333333332\n"
          ]
        }
      ],
      "source": [
        "knn_pipeline = Pipeline(steps=[\n",
        "    (\"imputer\", KNNImputer(n_neighbors=5)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "scores_knn = cross_val_score(\n",
        "    knn_pipeline,\n",
        "    X_train[num_features],\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "print(\"KNN imputation + pipeline CV accuracy:\", scores_knn.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwHgMp-AOfTM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

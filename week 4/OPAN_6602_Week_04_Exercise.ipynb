{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4 Exercise\n",
        "\n",
        "Let's put our examples of classifying Chinstrap vs. the rest into a full ML workflow.\n",
        "Below, look for chunks of text giving you instruction on filling out missing code."
      ],
      "metadata": {
        "id": "e7nIg9ndow7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyn37H20QulN"
      },
      "outputs": [],
      "source": [
        "### Step 0: Setup ----\n",
        "\n",
        "# If needed in a fresh Colab:\n",
        "# !pip install seaborn mlxtend statsmodels\n",
        "\n",
        "import numpy as np # all things math\n",
        "import pandas as pd # data frames, our workhorse for data\n",
        "\n",
        "# libraries for graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# sklearn packages\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# for feature selection\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Preprocessing ----\n",
        "\n",
        "# We’ll use Chinstrap vs. the rest only so ROC/AUC is clean binary logistic.\n",
        "\n",
        "# With pipelines, we don’t one-hot encode manually here.\n",
        "# We specify the encoder in the preprocessing pipeline,\n",
        "# and it will be fit only on the training data.\n",
        "\n",
        "# Load PalmerPenguins from seaborn\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "\n",
        "penguins.info()\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(penguins.isna().sum())\n"
      ],
      "metadata": {
        "id": "Rpffmsn3Rinw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up features and target, can do here or in feature engineering\n",
        "\n",
        "# Numeric features (all have some missingness)\n",
        "num_features = [\n",
        "    \"bill_length_mm\",\n",
        "    \"bill_depth_mm\",\n",
        "    \"flipper_length_mm\",\n",
        "    \"body_mass_g\"\n",
        "]\n",
        "\n",
        "# Categorical feature (also has missing values)\n",
        "cat_features = [\"sex\"]\n",
        "\n",
        "# Outcome: species, which we'll convert to 0/1\n",
        "target = \"species\"\n",
        "\n",
        "# Drop rows where the target is missing (just to be safe)\n",
        "penguins = penguins.dropna(subset=[target])\n",
        "\n",
        "# Define X and y\n",
        "X = penguins[num_features + cat_features]\n",
        "\n",
        "# Only predicting one species vs others for simplicity\n",
        "# Binary encode: Gentoo = 1, Adelie = 0\n",
        "y = (penguins[target] == \"Chinstrap\").astype(int)\n",
        "\n",
        "print(\"\\nX summary statistics:\")\n",
        "print(X.describe())\n",
        "\n",
        "print(\"\\ny summary statistics:\")\n",
        "print(y.describe())\n"
      ],
      "metadata": {
        "id": "bjZivxgOR0zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. Train/Test Split ----\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9lbQ-XRAUxI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3. Exploratory Data Analysis ----\n",
        "\n",
        "# We'll drop missing values before plotting\n",
        "\n",
        "train_eda = X_train.copy()\n",
        "train_eda[\"species\"] = np.where(y_train == 1, \"Gentoo\", \"Adelie\")\n",
        "\n",
        "\n",
        "sns.pairplot( # Drop rows with any missing values so seaborn doesn't complain\n",
        "    data=train_eda.dropna(subset=num_features + [\"species\"]),\n",
        "    vars=num_features,\n",
        "    hue=\"species\",\n",
        "    diag_kind=\"kde\",\n",
        "    corner=True # speed up plotting time by only looking at one triangle\n",
        ")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NhfvtICrU1pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, build pipelines for numeric and categorical variables that:\n",
        "\n",
        "1. Imputes missing values using `KNNImputer` for numeric variables and `SimpleImputer` to use most frequent values for categorical variables.\n",
        "2. Standardizes numeric variables.\n",
        "3. Encodes categorical variables as binary using `OneHotEncoder`.\n",
        "\n",
        "Use the `Pipeline` and `ColumnTransformer` functions to put this together. Apply to both the training and test sets."
      ],
      "metadata": {
        "id": "onJ5NLFQuMyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. Feature Engineering ----\n",
        "\n",
        "# Here we build a preprocessor that:\n",
        "\n",
        "# Applies KNNImputer + StandardScaler to numeric features\n",
        "# Applies SimpleImputer + OneHotEncoder to sex\n",
        "# We’ll use this preprocessor once to create the design matrix for modeling.\n",
        "\n",
        "# Numeric transformer: KNNImputer + StandardScaler\n",
        "numeric_transformer = [] # remove brackets and fill in the blank\n",
        "\n",
        "# Categorical transformer: most frequent imputation + one-hot encoding\n",
        "categorical_transformer = [] # remove brackets and fill in the blank\n",
        "\n",
        "# ColumnTransformer to apply to the right columns\n",
        "# Set up a dictonary of tuples specifying what transformations to apply\n",
        "# to which colums\n",
        "preprocessor = [] # remove brackets and fill in the blank\n",
        "\n",
        "# Fit on training data\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# this seamlessly applies to test data\n",
        "X_test_proc  = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Processed train shape:\", X_train_proc.shape)\n",
        "print(\"Processed test shape:\", X_test_proc.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "IcXD6QkbVH72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names after preprocessing (useful for stepwise and statsmodels later):\n",
        "\n",
        "# Feature names from the preprocessor\n",
        "num_feature_names = num_features\n",
        "\n",
        "# Getting categorical feature names is more complex because of the binary encoding\n",
        "cat_pipeline = preprocessor.named_transformers_[\"cat\"]\n",
        "ohe = cat_pipeline.named_steps[\"onehot\"]\n",
        "cat_feature_names = ohe.get_feature_names_out(cat_features)\n",
        "\n",
        "feature_names = np.concatenate([num_feature_names, cat_feature_names])\n",
        "print(\"Feature names after preprocessing:\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "id": "L77HpI7raD7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `StratifiedKFold` to create a cross validation object with 10 folds, stratified by the outcome variable. Set `random_state` to 42"
      ],
      "metadata": {
        "id": "v1ZJl_4DwEDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 5. Modeling ----\n",
        "\n",
        "# We will:\n",
        "# 1. Fit a “kitchen sink” logistic model using all processed features.\n",
        "# 2. Use stepwise (SFS) to select a subset of features.\n",
        "# 3. Evaluate both models using 10-fold cross-validation AUC on the training set.\n",
        "\n",
        "\n",
        "# Define CV sets\n",
        "cv = [] # remove brackets and fill in the blank\n"
      ],
      "metadata": {
        "id": "LfI8PXeiaILO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill out the arguments of `cross_val_score` to perform 10-fold cross validation on your logistic regression model. This is classification, so use `'roc_auc'` as the evaluation metric."
      ],
      "metadata": {
        "id": "3HQg1fpZwWtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kitchen sink model\n",
        "log_reg_full = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000\n",
        ")\n",
        "\n",
        "scores_full = c[] # remove brackets and fill in the blank\n",
        "\n",
        "print(\"Full model CV AUC scores:\", scores_full)\n",
        "print(\"Full model mean AUC:\", scores_full.mean())"
      ],
      "metadata": {
        "id": "zeEIgAigaySt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stepwise selection using SFS (on preprocessed training data)\n",
        "sfs = SFS(\n",
        "    estimator=LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=2000\n",
        "    ),\n",
        "    k_features=\"best\",          # let SFS choose the best number of features\n",
        "    forward=True,\n",
        "    floating=True,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "sfs = sfs.fit(X_train_proc, y_train)\n",
        "\n",
        "selected_idx = list(sfs.k_feature_idx_)\n",
        "selected_feature_names = feature_names[selected_idx]\n",
        "\n",
        "print(\"\\nSelected feature indices:\", selected_idx)\n",
        "print(\"Selected feature names:\", selected_feature_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "jPd24P6Nay91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select out only columns picked by stepwise selection\n",
        "X_train_step = X_train_proc[:, selected_idx]\n",
        "X_test_step  = X_test_proc[:, selected_idx]\n"
      ],
      "metadata": {
        "id": "rYP5Ud1XbIjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, use `cross_val_score` to perform 10-fold cross validation on the model selected by `SFS`, above. Note that we have subset the X matrices (test and train) based on the selected variables in the chunk above."
      ],
      "metadata": {
        "id": "fCpZ354lw7I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the stepwise model on the 10-fold CV set\n",
        "log_reg_step = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000\n",
        ")\n",
        "\n",
        "scores_step = [] # remove brackets and fill in the blank\n",
        "\n",
        "print(\"\\nStepwise model CV AUC scores:\", scores_step)\n",
        "print(\"Stepwise model mean AUC:\", scores_step.mean())\n"
      ],
      "metadata": {
        "id": "Vd3FhKPRbPzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 6. Evaluation on the test set ----\n",
        "\n",
        "# Fit both models on all training data\n",
        "log_reg_full.fit(X_train_proc, y_train)\n",
        "log_reg_step.fit(X_train_step, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_prob_full = log_reg_full.predict_proba(X_test_proc)[:, 1]\n",
        "y_prob_step = log_reg_step.predict_proba(X_test_step)[:, 1]\n",
        "\n",
        "auc_full_test = roc_auc_score(y_test, y_prob_full)\n",
        "auc_step_test = roc_auc_score(y_test, y_prob_step)\n",
        "\n",
        "print(f\"\\nTest AUC - Full model:     {auc_full_test:.3f}\")\n",
        "print(f\"Test AUC - Stepwise model: {auc_step_test:.3f}\")\n"
      ],
      "metadata": {
        "id": "x6P6bdKOeeBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot both ROC curves\n",
        "fpr_full, tpr_full, _ = roc_curve(y_test, y_prob_full)\n",
        "fpr_step, tpr_step, _ = roc_curve(y_test, y_prob_step)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr_full, tpr_full, label=f\"Full model (AUC={auc_full_test:.3f})\")\n",
        "plt.plot(fpr_step, tpr_step, label=f\"Stepwise model (AUC={auc_step_test:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves on Test Set\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p1GEO8YDejE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Bonus: Refit the stepwise model in statsmodels for inference ----\n",
        "# 1. Take the preprocessed training data\n",
        "# 2. Keep only the selected columns\n",
        "# 3. Fit a statsmodels.Logit model for inference (coefficients, standard errors, etc.)\n",
        "\n",
        "# Build a DataFrame for statsmodels with selected features\n",
        "X_train_step_df = pd.DataFrame(\n",
        "    X_train_step,\n",
        "    columns=selected_feature_names,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_step_df = pd.DataFrame(\n",
        "    X_test_step,\n",
        "    columns=selected_feature_names,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "# Add intercept\n",
        "X_train_sm = sm.add_constant(X_train_step_df)\n",
        "X_test_sm  = sm.add_constant(X_test_step_df, has_constant=\"add\")\n",
        "\n",
        "# y must be aligned and numeric (already 0/1)\n",
        "logit_sm = sm.Logit(y_train, X_train_sm).fit()\n",
        "print(logit_sm.summary())\n"
      ],
      "metadata": {
        "id": "IeoxYsLReq_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aG46cQ-Ne3qS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}